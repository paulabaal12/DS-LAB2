{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bc909c",
   "metadata": {},
   "source": [
    "### LABORATORIO #2 \n",
    "\n",
    "- Diego Duarte\n",
    "- Paula Barilals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ee5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento: 60000\n",
      "Datos de prueba: 10000\n",
      "Forma de X_entreno: (60000, 28, 28)\n",
      "Forma de X_prueba: (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Imprimir cantidad de datos de entrenamiento y prueba\n",
    "print(f\"Datos de entrenamiento: {X_entreno.shape[0]}\")\n",
    "print(f\"Datos de prueba: {X_prueba.shape[0]}\")\n",
    "\n",
    "# Mostrar forma de los datos\n",
    "print(f\"Forma de X_entreno: {X_entreno.shape}\")\n",
    "print(f\"Forma de X_prueba: {X_prueba.shape}\")\n",
    "\n",
    "# Preprocesamiento: normalizar imágenes a rango [0, 1]\n",
    "X_entreno = X_entreno.astype('float32') / 255.0\n",
    "X_prueba = X_prueba.astype('float32') / 255.0\n",
    "\n",
    "# Convertir etiquetas a one-hot encoding\n",
    "y_entreno = tf.keras.utils.to_categorical(y_entreno, 10)\n",
    "y_prueba = tf.keras.utils.to_categorical(y_prueba, 10)\n",
    "\n",
    "# ejemplo de imagen y etiqueta\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_entreno[0], cmap='gray')\n",
    "plt.title(f\"Etiqueta: {np.argmax(y_entreno[0])}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006f77d",
   "metadata": {},
   "source": [
    "### 1. Modificación del Ancho de la Red (8 puntos)\n",
    "- Modifique el tamaño de la capa escondida a 200 neuronas.\n",
    "- Experimente con diferentes tamaños de capa escondida (50, 100, 300, 500) y determine\n",
    "cuál ofrece el mejor rendimiento\n",
    "\n",
    "1. ¿Cómo cambia la precisión de validación del modelo?\n",
    "\n",
    "2. ¿Cuánto tiempo tarda el algoritmo en entrenar?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750e7e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa escondida: 50 neuronas\n",
      "  Precisión de validación final: 0.9640\n",
      "  Pérdida de validación final: 0.1209\n",
      "----------------------------------------\n",
      "Capa escondida: 100 neuronas\n",
      "  Precisión de validación final: 0.9708\n",
      "  Pérdida de validación final: 0.0962\n",
      "----------------------------------------\n",
      "Capa escondida: 200 neuronas\n",
      "  Precisión de validación final: 0.9766\n",
      "  Pérdida de validación final: 0.0748\n",
      "----------------------------------------\n",
      "Capa escondida: 300 neuronas\n",
      "  Precisión de validación final: 0.9759\n",
      "  Pérdida de validación final: 0.0752\n",
      "----------------------------------------\n",
      "Capa escondida: 500 neuronas\n",
      "  Precisión de validación final: 0.9807\n",
      "  Pérdida de validación final: 0.0650\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# crear y entrenar el modelo con diferente tamaño de capa escondida\n",
    "def entrenar_modelo(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128, \n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Capa escondida: {hidden_units} neuronas\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# Probar diferentes tamaños de capa escondida\n",
    "for unidades in [50, 100, 200, 300, 500]:\n",
    "    entrenar_modelo(unidades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b46ae1",
   "metadata": {},
   "source": [
    "### 2. Modificación de la Profundidad de la Red\n",
    "Agregue una capa escondida adicional al modelo.\n",
    "- Documente cuidadosamente las dimensiones de los pesos y sesgos\n",
    "- Compare la precisión de validación con el modelo original\n",
    "\n",
    "1. Analice el impacto en el tiempo de ejecución\n",
    "2. Explique los cambios necesarios en el código para implementar esta modificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2947c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capas escondidas: 200 y 100 neuronas\n",
      "  Precisión de validación final: 0.9777\n",
      "  Pérdida de validación final: 0.0748\n",
      "----------------------------------------\n",
      "Capa 0: pesos None, sesgos None\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 100), sesgos (100,)\n",
      "Capa 3: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9776999950408936, 0.07483813911676407)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Modelo con dos capas escondidas\n",
    "def entrenar_modelo2c(hidden_units1, hidden_units2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units1, activation='relu'),\n",
    "        layers.Dense(hidden_units2, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Capas escondidas: {hidden_units1} y {hidden_units2} neuronas\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    # Dimensiones de pesos y sesgos\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'weights'):\n",
    "            pesos = layer.get_weights()[0].shape if layer.get_weights() else None\n",
    "            sesgos = layer.get_weights()[1].shape if layer.get_weights() else None\n",
    "            print(f\"Capa {i}: pesos {pesos}, sesgos {sesgos}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# prueba con 200 y 100 neuronas\n",
    "entrenar_modelo2c(200, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ca9d1",
   "metadata": {},
   "source": [
    "### 3. Redes Profundas\n",
    "Experimente con arquitecturas más profundas, llegando hasta 5 capas escondidas.\n",
    "- Ajuste el ancho de cada capa según considere conveniente\n",
    "- Documente la precisión de validación para cada configuración\n",
    "- Analice la relación entre profundidad y tiempo de ejecución\n",
    "- Identifique posibles problemas de desvanecimiento del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0805bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura: [100] neuronas por capa\n",
      "  Precisión de validación final: 0.9709\n",
      "  Pérdida de validación final: 0.0986\n",
      "  Tiempo de entrenamiento: 7.50 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 100), sesgos (100,)\n",
      "Capa 2: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [200, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9767\n",
      "  Pérdida de validación final: 0.0821\n",
      "  Tiempo de entrenamiento: 11.35 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 100), sesgos (100,)\n",
      "Capa 3: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [200, 150, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9799\n",
      "  Pérdida de validación final: 0.0696\n",
      "  Tiempo de entrenamiento: 10.80 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 150), sesgos (150,)\n",
      "Capa 3: pesos (150, 100), sesgos (100,)\n",
      "Capa 4: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [300, 200, 150, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9763\n",
      "  Pérdida de validación final: 0.0817\n",
      "  Tiempo de entrenamiento: 15.00 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 300), sesgos (300,)\n",
      "Capa 2: pesos (300, 200), sesgos (200,)\n",
      "Capa 3: pesos (200, 150), sesgos (150,)\n",
      "Capa 4: pesos (150, 100), sesgos (100,)\n",
      "Capa 5: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [500, 400, 300, 200, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9802\n",
      "  Pérdida de validación final: 0.0697\n",
      "  Tiempo de entrenamiento: 54.95 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 500), sesgos (500,)\n",
      "Capa 2: pesos (500, 400), sesgos (400,)\n",
      "Capa 3: pesos (400, 300), sesgos (300,)\n",
      "Capa 4: pesos (300, 200), sesgos (200,)\n",
      "Capa 5: pesos (200, 100), sesgos (100,)\n",
      "Capa 6: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Función para entrenar modelos con hasta 5 capas escondidas\n",
    "def entrenar_red(hidden_units_list):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for units in hidden_units_list:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Arquitectura: {hidden_units_list} neuronas por capa\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    # dimensiones de pesos y sesgos\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'weights') and layer.get_weights():\n",
    "            pesos = layer.get_weights()[0].shape\n",
    "            sesgos = layer.get_weights()[1].shape\n",
    "            print(f\"Capa {i}: pesos {pesos}, sesgos {sesgos}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# arquitecturas profundas\n",
    "arquitecturas = [\n",
    "    [100],\n",
    "    [200, 100],\n",
    "    [200, 150, 100],\n",
    "    [300, 200, 150, 100],\n",
    "    [500, 400, 300, 200, 100]\n",
    "]\n",
    "\n",
    "for config in arquitecturas:\n",
    "    entrenar_red(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee60ef",
   "metadata": {},
   "source": [
    "### 4. Funciones de Activación I \n",
    "Aplique la función de activación sigmoidal a todas las capas.\n",
    "- Compare el rendimiento con las activaciones originales\n",
    "- Analice el impacto en la velocidad de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8f8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparando para 50 neuronas en la capa escondida:\n",
      "[ReLU] 50 neuronas - Precisión: 0.9639 - Pérdida: 0.1235 - Tiempo: 6.09s\n",
      "[Sigmoide] 50 neuronas - Precisión: 0.9454 - Pérdida: 0.1918 - Tiempo: 6.86s\n",
      "\n",
      "Comparando para 100 neuronas en la capa escondida:\n",
      "[ReLU] 100 neuronas - Precisión: 0.9733 - Pérdida: 0.0915 - Tiempo: 6.66s\n",
      "[Sigmoide] 100 neuronas - Precisión: 0.9525 - Pérdida: 0.1627 - Tiempo: 8.07s\n",
      "\n",
      "Comparando para 200 neuronas en la capa escondida:\n",
      "[ReLU] 200 neuronas - Precisión: 0.9741 - Pérdida: 0.0841 - Tiempo: 9.20s\n",
      "[Sigmoide] 200 neuronas - Precisión: 0.9556 - Pérdida: 0.1505 - Tiempo: 9.27s\n",
      "\n",
      "Comparando para 300 neuronas en la capa escondida:\n",
      "[ReLU] 300 neuronas - Precisión: 0.9760 - Pérdida: 0.0776 - Tiempo: 10.97s\n",
      "[Sigmoide] 300 neuronas - Precisión: 0.9616 - Pérdida: 0.1309 - Tiempo: 13.23s\n",
      "\n",
      "Comparando para 500 neuronas en la capa escondida:\n",
      "[ReLU] 500 neuronas - Precisión: 0.9777 - Pérdida: 0.0685 - Tiempo: 13.58s\n",
      "[Sigmoide] 500 neuronas - Precisión: 0.9638 - Pérdida: 0.1234 - Tiempo: 15.03s\n"
     ]
    }
   ],
   "source": [
    "# Función para entrenar el modelo usando activación ReLU\n",
    "def entrenar_relu(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[ReLU] {hidden_units} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Función para entrenar el modelo usando activación sigmoidal\n",
    "def entrenar_sigmoide(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='sigmoid'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[Sigmoide] {hidden_units} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Comparar rendimiento y velocidad de convergencia\n",
    "for unidades in [50, 100, 200, 300, 500]:\n",
    "    print(f\"\\nComparando para {unidades} neuronas en la capa escondida:\")\n",
    "    entrenar_relu(unidades)\n",
    "    entrenar_sigmoide(unidades)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abe336",
   "metadata": {},
   "source": [
    "### 5. Funciones de Activación II \n",
    "Aplique ReLU a la primera capa escondida y tanh a la segunda\n",
    "- Compare el rendimiento con las configuraciones anteriores\n",
    "- Explique las ventajas y desventajas de cada función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493d0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparando para 200 y 100 neuronas:\n",
      "[ReLU-Tanh] 200 y 100 neuronas - Precisión: 0.9764 - Pérdida: 0.0734 - Tiempo: 10.77s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 300 y 200 neuronas:\n",
      "[ReLU-Tanh] 300 y 200 neuronas - Precisión: 0.9799 - Pérdida: 0.0687 - Tiempo: 11.73s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 500 y 300 neuronas:\n",
      "[ReLU-Tanh] 500 y 300 neuronas - Precisión: 0.9770 - Pérdida: 0.0793 - Tiempo: 20.26s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 500 y 500 neuronas:\n",
      "[ReLU-Tanh] 500 y 500 neuronas - Precisión: 0.9789 - Pérdida: 0.0813 - Tiempo: 19.90s\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Modelo con ReLU en la primera capa escondida y tanh en la segunda\n",
    "def ReLU_tanh(hidden_units1, hidden_units2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units1, activation='relu'),\n",
    "        layers.Dense(hidden_units2, activation='tanh'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[ReLU-Tanh] {hidden_units1} y {hidden_units2} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Comparar con diferentes cantidades de neuronas\n",
    "configs = [(200, 100), (300, 200), (500, 300), (500, 500)]\n",
    "for h1, h2 in configs:\n",
    "    print(f\"\\nComparando para {h1} y {h2} neuronas:\")\n",
    "    ReLU_tanh(h1, h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d0447",
   "metadata": {},
   "source": [
    "### 6. Tamaño de Batch Grande\n",
    "Modifique el tamaño de batch a 10,000.\n",
    "- Documente el cambio en el tiempo de entrenamiento\n",
    "- Analice el impacto en la precisión del modelo\n",
    "- Explique teóricamente por qué se observan estos cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c0bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10000 | Neuronas: 200\n",
      "  Precisión de validación final: 0.8936\n",
      "  Pérdida de validación final: 0.3950\n",
      "  Tiempo de entrenamiento: 2.41 segundos\n",
      "----------------------------------------\n",
      "Batch size: 10000 | Neuronas: 500\n",
      "  Precisión de validación final: 0.9100\n",
      "  Pérdida de validación final: 0.3213\n",
      "  Tiempo de entrenamiento: 3.01 segundos\n",
      "----------------------------------------\n",
      "Batch size: 10000 | Neuronas: 1000\n",
      "  Precisión de validación final: 0.9229\n",
      "  Pérdida de validación final: 0.2736\n",
      "  Tiempo de entrenamiento: 4.54 segundos\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9229000210762024, 0.2735738754272461, 4.5422632694244385)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entrenar_batch_grande(hidden_units, batch_size=10000):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=batch_size,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Batch size: {batch_size} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y batch grande\n",
    "entrenar_batch_grande(200, batch_size = 10000)\n",
    "# 500 neuronas y batch grande\n",
    "entrenar_batch_grande(500, batch_size = 10000)\n",
    "# 1000 neuronas y batch grande\n",
    "entrenar_batch_grande(1000, batch_size = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154b55f",
   "metadata": {},
   "source": [
    "### 7. Descenso de Gradiente Estocástico (SGD)\n",
    "Ajuste el tamaño de batch a 1 (SGD puro).\n",
    "- Compare el tiempo de ejecución con configuraciones anteriores\n",
    "- Analice la estabilidad y precisión del entrenamiento\n",
    "- Explique si los resultados son coherentes con la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84041d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val_acc, val_loss, end-start\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 200 neuronas y batch=1 (SGD puro)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mentrenar_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 500 neuronas y batch=1 (SGD puro)\u001b[39;00m\n\u001b[32m     26\u001b[39m entrenar_sgd(\u001b[32m500\u001b[39m, batch_size = \u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mentrenar_sgd\u001b[39m\u001b[34m(hidden_units, batch_size)\u001b[39m\n\u001b[32m      7\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m               loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m               metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_entreno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_entreno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_prueba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prueba\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m end = time.time()\n\u001b[32m     14\u001b[39m val_acc = history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:401\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    392\u001b[39m         x=val_x,\n\u001b[32m    393\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    400\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m val_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m val_logs = {\n\u001b[32m    412\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[33m\"\u001b[39m + name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    413\u001b[39m }\n\u001b[32m    414\u001b[39m epoch_logs.update(val_logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:489\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    488\u001b[39m     callbacks.on_test_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     callbacks.on_test_batch_end(step, logs)\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_evaluating:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:221\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m ):\n\u001b[32m    220\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\OneDrive\\Escritorio\\2025\\Semestre VIII\\DataScience\\tf2-env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def entrenar_sgd(hidden_units, batch_size=1):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=batch_size,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Batch size: {batch_size} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(200, batch_size = 1)\n",
    "# 500 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(500, batch_size = 1)\n",
    "# 1000 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(1000, batch_size = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536f6e7",
   "metadata": {},
   "source": [
    "### 8. Tasa de Aprendizaje Baja\n",
    "Modifique la tasa de aprendizaje a 0.0001.\n",
    "- Documente el impacto en la convergencia del modelo\n",
    "- Analice si el modelo alcanza mejor precisión o se queda atrapado en mínimos locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef24eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de aprendizaje: 0.0001 | Neuronas: 200\n",
      "  Precisión de validación final: 0.9401\n",
      "  Pérdida de validación final: 0.2165\n",
      "  Tiempo de entrenamiento: 12.75 segundos\n",
      "----------------------------------------\n",
      "Tasa de aprendizaje: 0.0001 | Neuronas: 500\n",
      "  Precisión de validación final: 0.9516\n",
      "  Pérdida de validación final: 0.1699\n",
      "  Tiempo de entrenamiento: 14.26 segundos\n",
      "----------------------------------------\n",
      "Tasa de aprendizaje: 0.0001 | Neuronas: 1000\n",
      "  Precisión de validación final: 0.9600\n",
      "  Pérdida de validación final: 0.1400\n",
      "  Tiempo de entrenamiento: 18.59 segundos\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9599999785423279, 0.14000888168811798, 18.591599702835083)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entrenar_lr_bajo(hidden_units, lr=0.0001):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Tasa de aprendizaje: {lr} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(200, lr = 0.0001)\n",
    "# 500 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(500, lr = 0.0001)\n",
    "# 1000 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(1000, lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dad08",
   "metadata": {},
   "source": [
    "### 9. Tasa de Aprendizaje Alta \n",
    "Ajuste la tasa de aprendizaje a 0.02.\n",
    "- Documente el impacto en la estabilidad del entrenamiento\n",
    "- Analice si se produce divergencia o mejora en la velocidad de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62edffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de aprendizaje: 0.02 | Neuronas: 200\n",
      "  Precisión de validación final: 0.9505\n",
      "  Pérdida de validación final: 0.2266\n",
      "  Tiempo de entrenamiento: 10.40 segundos\n",
      "----------------------------------------\n",
      "Tasa de aprendizaje: 0.02 | Neuronas: 500\n",
      "  Precisión de validación final: 0.9586\n",
      "  Pérdida de validación final: 0.1949\n",
      "  Tiempo de entrenamiento: 12.98 segundos\n",
      "----------------------------------------\n",
      "Tasa de aprendizaje: 0.02 | Neuronas: 1000\n",
      "  Precisión de validación final: 0.9540\n",
      "  Pérdida de validación final: 0.2219\n",
      "  Tiempo de entrenamiento: 19.69 segundos\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9539999961853027, 0.2218923270702362, 19.6949360370636)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def entrenar_lr_alta(hidden_units, lr=0.02):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Tasa de aprendizaje: {lr} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(200, lr=0.02)\n",
    "# 500 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(500, lr=0.02)\n",
    "# 1000 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(1000, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb7ae3",
   "metadata": {},
   "source": [
    "### 10. Optimización Avanzada (10 puntos)\n",
    "Implemente técnicas de regularización:\n",
    "- Agregue dropout entre capas (pruebe diferentes tasas)\n",
    "- Experimente con regularización L2\n",
    "- Documente el impacto en la generalización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32b72e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout: 0.3 | L2: 0.0000 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9725\n",
      "  Precisión validación final:  0.9760\n",
      "  Pérdida validación final:    0.0785\n",
      "  Tiempo de entrenamiento:     12.02 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.3 | L2: 0.0010 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9603\n",
      "  Precisión validación final:  0.9693\n",
      "  Pérdida validación final:    0.1795\n",
      "  Tiempo de entrenamiento:     10.77 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.3 | L2: 0.0100 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9333\n",
      "  Precisión validación final:  0.9502\n",
      "  Pérdida validación final:    0.3109\n",
      "  Tiempo de entrenamiento:     11.26 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.5 | L2: 0.0000 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9618\n",
      "  Precisión validación final:  0.9728\n",
      "  Pérdida validación final:    0.0883\n",
      "  Tiempo de entrenamiento:     32.44 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.5 | L2: 0.0010 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9498\n",
      "  Precisión validación final:  0.9663\n",
      "  Pérdida validación final:    0.2014\n",
      "  Tiempo de entrenamiento:     32.69 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.5 | L2: 0.0100 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9225\n",
      "  Precisión validación final:  0.9493\n",
      "  Pérdida validación final:    0.3326\n",
      "  Tiempo de entrenamiento:     18.13 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.7 | L2: 0.0000 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9380\n",
      "  Precisión validación final:  0.9657\n",
      "  Pérdida validación final:    0.1193\n",
      "  Tiempo de entrenamiento:     11.67 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.7 | L2: 0.0010 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.9245\n",
      "  Precisión validación final:  0.9609\n",
      "  Pérdida validación final:    0.2340\n",
      "  Tiempo de entrenamiento:     16.33 segundos\n",
      "--------------------------------------------------\n",
      "Dropout: 0.7 | L2: 0.0100 | Neuronas: 200\n",
      "  Precisión entrenamiento final: 0.8924\n",
      "  Precisión validación final:  0.9384\n",
      "  Pérdida validación final:    0.3873\n",
      "  Tiempo de entrenamiento:     27.81 segundos\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "def entrenar_con_regularizacion(hidden_units=200, dropout_rate=0.5, l2_lambda=0.001):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    \n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    \n",
    "    print(f\"Dropout: {dropout_rate} | L2: {l2_lambda:.4f} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión entrenamiento final: {train_acc:.4f}\")\n",
    "    print(f\"  Precisión validación final:  {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida validación final:    {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento:     {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 50)\n",
    "    return val_acc, val_loss, train_acc, end - start\n",
    "\n",
    "for dropout in [0.3, 0.5, 0.7]:\n",
    "    for l2_lambda in [0.0, 0.001, 0.01]:\n",
    "        entrenar_con_regularizacion(hidden_units=200, dropout_rate=dropout, l2_lambda=l2_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c1158",
   "metadata": {},
   "source": [
    "### 11. Visualización (5 puntos)\n",
    "Cree gráficos que muestren:\n",
    "- Evolución de la precisión y pérdida durante el entrenamiento\n",
    "- Comparación de rendimiento entre diferentes configuraciones\n",
    "- Visualización de algunos filtros o características aprendidas por la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_history(history, etiqueta=\"Modelo\"):\n",
    "    # Precisión\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "    plt.title(f'Precisión durante entrenamiento - {etiqueta}')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    # Pérdida\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Validación')\n",
    "    plt.title(f'Pérdida durante entrenamiento - {etiqueta}')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def comparar_histories(histories, etiquetas):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Comparar precisión validación\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for hist, etiq in zip(histories, etiquetas):\n",
    "        plt.plot(hist.history['val_accuracy'], label=etiq)\n",
    "    plt.title('Comparación de precisión de validación')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    # Comparar pérdida validación\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for hist, etiq in zip(histories, etiquetas):\n",
    "        plt.plot(hist.history['val_loss'], label=etiq)\n",
    "    plt.title('Comparación de pérdida de validación')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualizar_filtros_primera_capa(model, num_filtros=10):\n",
    "    # Obtiene la primera capa densa\n",
    "    primera_capa = None\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Dense):\n",
    "            primera_capa = layer\n",
    "            break\n",
    "    if primera_capa is None:\n",
    "        print(\"No se encontró una capa Dense en el modelo.\")\n",
    "        return\n",
    "    \n",
    "    pesos, biases = primera_capa.get_weights()\n",
    "    print(f\"Pesos forma: {pesos.shape} (entrada x neuronas)\")\n",
    "\n",
    "    # Mostrar pesos de los primeros 'num_filtros' neuronas\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(min(num_filtros, pesos.shape[1])):\n",
    "        plt.subplot(1, num_filtros, i+1)\n",
    "        filtro = pesos[:, i].reshape(28, 28)  # suponiendo entrada 28x28\n",
    "        plt.imshow(filtro, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Filtro {i+1}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bfe74",
   "metadata": {},
   "source": [
    "### 12. Modelo Óptimo (14 puntos)\n",
    "Combine todos los métodos anteriores para diseñar un modelo que:\n",
    "- Alcance una precisión de validación de 98.5% o superior\n",
    "- Optimice el tiempo de entrenamiento\n",
    "- Presente la arquitectura más eficiente posible\n",
    "- Justifique cada decisión de diseño"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
