{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bc909c",
   "metadata": {},
   "source": [
    "### LABORATORIO #2 \n",
    "\n",
    "- Diego Duarte\n",
    "- Paula Barilals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ee5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento: 60000\n",
      "Datos de prueba: 10000\n",
      "Forma de X_entreno: (60000, 28, 28)\n",
      "Forma de X_prueba: (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Imprimir cantidad de datos de entrenamiento y prueba\n",
    "print(f\"Datos de entrenamiento: {X_entreno.shape[0]}\")\n",
    "print(f\"Datos de prueba: {X_prueba.shape[0]}\")\n",
    "\n",
    "# Mostrar forma de los datos\n",
    "print(f\"Forma de X_entreno: {X_entreno.shape}\")\n",
    "print(f\"Forma de X_prueba: {X_prueba.shape}\")\n",
    "\n",
    "# Preprocesamiento: normalizar imágenes a rango [0, 1]\n",
    "X_entreno = X_entreno.astype('float32') / 255.0\n",
    "X_prueba = X_prueba.astype('float32') / 255.0\n",
    "\n",
    "# Convertir etiquetas a one-hot encoding\n",
    "y_entreno = tf.keras.utils.to_categorical(y_entreno, 10)\n",
    "y_prueba = tf.keras.utils.to_categorical(y_prueba, 10)\n",
    "\n",
    "# ejemplo de imagen y etiqueta\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_entreno[0], cmap='gray')\n",
    "plt.title(f\"Etiqueta: {np.argmax(y_entreno[0])}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006f77d",
   "metadata": {},
   "source": [
    "### 1. Modificación del Ancho de la Red (8 puntos)\n",
    "- Modifique el tamaño de la capa escondida a 200 neuronas.\n",
    "- Experimente con diferentes tamaños de capa escondida (50, 100, 300, 500) y determine\n",
    "cuál ofrece el mejor rendimiento\n",
    "\n",
    "1. ¿Cómo cambia la precisión de validación del modelo?\n",
    "\n",
    "2. ¿Cuánto tiempo tarda el algoritmo en entrenar?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750e7e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa escondida: 50 neuronas\n",
      "  Precisión de validación final: 0.9614\n",
      "  Pérdida de validación final: 0.1261\n",
      "----------------------------------------\n",
      "Capa escondida: 100 neuronas\n",
      "  Precisión de validación final: 0.9704\n",
      "  Pérdida de validación final: 0.1000\n",
      "----------------------------------------\n",
      "Capa escondida: 200 neuronas\n",
      "  Precisión de validación final: 0.9783\n",
      "  Pérdida de validación final: 0.0782\n",
      "----------------------------------------\n",
      "Capa escondida: 300 neuronas\n",
      "  Precisión de validación final: 0.9781\n",
      "  Pérdida de validación final: 0.0735\n",
      "----------------------------------------\n",
      "Capa escondida: 500 neuronas\n",
      "  Precisión de validación final: 0.9778\n",
      "  Pérdida de validación final: 0.0647\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# crear y entrenar el modelo con diferente tamaño de capa escondida\n",
    "def entrenar_modelo(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128, \n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Capa escondida: {hidden_units} neuronas\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# Probar diferentes tamaños de capa escondida\n",
    "for unidades in [50, 100, 200, 300, 500]:\n",
    "    entrenar_modelo(unidades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b46ae1",
   "metadata": {},
   "source": [
    "### 2. Modificación de la Profundidad de la Red\n",
    "Agregue una capa escondida adicional al modelo.\n",
    "- Documente cuidadosamente las dimensiones de los pesos y sesgos\n",
    "- Compare la precisión de validación con el modelo original\n",
    "\n",
    "1. Analice el impacto en el tiempo de ejecución\n",
    "2. Explique los cambios necesarios en el código para implementar esta modificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2947c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capas escondidas: 200 y 100 neuronas\n",
      "  Precisión de validación final: 0.9761\n",
      "  Pérdida de validación final: 0.0721\n",
      "----------------------------------------\n",
      "Capa 0: pesos None, sesgos None\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 100), sesgos (100,)\n",
      "Capa 3: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9761000275611877, 0.07214768975973129)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Modelo con dos capas escondidas\n",
    "def entrenar_modelo2c(hidden_units1, hidden_units2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units1, activation='relu'),\n",
    "        layers.Dense(hidden_units2, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Capas escondidas: {hidden_units1} y {hidden_units2} neuronas\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    # Dimensiones de pesos y sesgos\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'weights'):\n",
    "            pesos = layer.get_weights()[0].shape if layer.get_weights() else None\n",
    "            sesgos = layer.get_weights()[1].shape if layer.get_weights() else None\n",
    "            print(f\"Capa {i}: pesos {pesos}, sesgos {sesgos}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# prueba con 200 y 100 neuronas\n",
    "entrenar_modelo2c(200, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ca9d1",
   "metadata": {},
   "source": [
    "### 3. Redes Profundas\n",
    "Experimente con arquitecturas más profundas, llegando hasta 5 capas escondidas.\n",
    "- Ajuste el ancho de cada capa según considere conveniente\n",
    "- Documente la precisión de validación para cada configuración\n",
    "- Analice la relación entre profundidad y tiempo de ejecución\n",
    "- Identifique posibles problemas de desvanecimiento del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0805bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura: [100] neuronas por capa\n",
      "  Precisión de validación final: 0.9698\n",
      "  Pérdida de validación final: 0.0970\n",
      "  Tiempo de entrenamiento: 17.32 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 100), sesgos (100,)\n",
      "Capa 2: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [200, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9735\n",
      "  Pérdida de validación final: 0.0805\n",
      "  Tiempo de entrenamiento: 20.84 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 100), sesgos (100,)\n",
      "Capa 3: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [200, 150, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9731\n",
      "  Pérdida de validación final: 0.0893\n",
      "  Tiempo de entrenamiento: 24.25 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 200), sesgos (200,)\n",
      "Capa 2: pesos (200, 150), sesgos (150,)\n",
      "Capa 3: pesos (150, 100), sesgos (100,)\n",
      "Capa 4: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [300, 200, 150, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9762\n",
      "  Pérdida de validación final: 0.0741\n",
      "  Tiempo de entrenamiento: 24.88 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 300), sesgos (300,)\n",
      "Capa 2: pesos (300, 200), sesgos (200,)\n",
      "Capa 3: pesos (200, 150), sesgos (150,)\n",
      "Capa 4: pesos (150, 100), sesgos (100,)\n",
      "Capa 5: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n",
      "Arquitectura: [500, 400, 300, 200, 100] neuronas por capa\n",
      "  Precisión de validación final: 0.9788\n",
      "  Pérdida de validación final: 0.0767\n",
      "  Tiempo de entrenamiento: 40.17 segundos\n",
      "----------------------------------------\n",
      "Capa 1: pesos (784, 500), sesgos (500,)\n",
      "Capa 2: pesos (500, 400), sesgos (400,)\n",
      "Capa 3: pesos (400, 300), sesgos (300,)\n",
      "Capa 4: pesos (300, 200), sesgos (200,)\n",
      "Capa 5: pesos (200, 100), sesgos (100,)\n",
      "Capa 6: pesos (100, 10), sesgos (10,)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Función para entrenar modelos con hasta 5 capas escondidas\n",
    "def entrenar_red(hidden_units_list):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for units in hidden_units_list:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Arquitectura: {hidden_units_list} neuronas por capa\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    # dimensiones de pesos y sesgos\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'weights') and layer.get_weights():\n",
    "            pesos = layer.get_weights()[0].shape\n",
    "            sesgos = layer.get_weights()[1].shape\n",
    "            print(f\"Capa {i}: pesos {pesos}, sesgos {sesgos}\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss\n",
    "\n",
    "# arquitecturas profundas\n",
    "arquitecturas = [\n",
    "    [100],\n",
    "    [200, 100],\n",
    "    [200, 150, 100],\n",
    "    [300, 200, 150, 100],\n",
    "    [500, 400, 300, 200, 100]\n",
    "]\n",
    "\n",
    "for config in arquitecturas:\n",
    "    entrenar_red(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee60ef",
   "metadata": {},
   "source": [
    "### 4. Funciones de Activación I \n",
    "Aplique la función de activación sigmoidal a todas las capas.\n",
    "- Compare el rendimiento con las activaciones originales\n",
    "- Analice el impacto en la velocidad de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8f8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparando para 50 neuronas en la capa escondida:\n",
      "[ReLU] 50 neuronas - Precisión: 0.9610 - Pérdida: 0.1271 - Tiempo: 13.20s\n",
      "[Sigmoide] 50 neuronas - Precisión: 0.9465 - Pérdida: 0.1874 - Tiempo: 12.77s\n",
      "\n",
      "Comparando para 100 neuronas en la capa escondida:\n",
      "[ReLU] 100 neuronas - Precisión: 0.9697 - Pérdida: 0.1017 - Tiempo: 15.14s\n",
      "[Sigmoide] 100 neuronas - Precisión: 0.9534 - Pérdida: 0.1577 - Tiempo: 19.65s\n",
      "\n",
      "Comparando para 200 neuronas en la capa escondida:\n",
      "[ReLU] 200 neuronas - Precisión: 0.9752 - Pérdida: 0.0765 - Tiempo: 20.19s\n",
      "[Sigmoide] 200 neuronas - Precisión: 0.9590 - Pérdida: 0.1370 - Tiempo: 19.70s\n",
      "\n",
      "Comparando para 300 neuronas en la capa escondida:\n",
      "[ReLU] 300 neuronas - Precisión: 0.9756 - Pérdida: 0.0770 - Tiempo: 19.09s\n",
      "[Sigmoide] 300 neuronas - Precisión: 0.9607 - Pérdida: 0.1277 - Tiempo: 19.26s\n",
      "\n",
      "Comparando para 500 neuronas en la capa escondida:\n",
      "[ReLU] 500 neuronas - Precisión: 0.9798 - Pérdida: 0.0645 - Tiempo: 22.06s\n",
      "[Sigmoide] 500 neuronas - Precisión: 0.9635 - Pérdida: 0.1200 - Tiempo: 23.13s\n"
     ]
    }
   ],
   "source": [
    "# Función para entrenar el modelo usando activación ReLU\n",
    "def entrenar_relu(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[ReLU] {hidden_units} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Función para entrenar el modelo usando activación sigmoidal\n",
    "def entrenar_sigmoide(hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='sigmoid'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[Sigmoide] {hidden_units} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Comparar rendimiento y velocidad de convergencia\n",
    "for unidades in [50, 100, 200, 300, 500]:\n",
    "    print(f\"\\nComparando para {unidades} neuronas en la capa escondida:\")\n",
    "    entrenar_relu(unidades)\n",
    "    entrenar_sigmoide(unidades)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abe336",
   "metadata": {},
   "source": [
    "### 5. Funciones de Activación II \n",
    "Aplique ReLU a la primera capa escondida y tanh a la segunda\n",
    "- Compare el rendimiento con las configuraciones anteriores\n",
    "- Explique las ventajas y desventajas de cada función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493d0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparando para 200 y 100 neuronas:\n",
      "[ReLU-Tanh] 200 y 100 neuronas - Precisión: 0.9793 - Pérdida: 0.0700 - Tiempo: 23.49s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 300 y 200 neuronas:\n",
      "[ReLU-Tanh] 300 y 200 neuronas - Precisión: 0.9780 - Pérdida: 0.0721 - Tiempo: 22.23s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 500 y 300 neuronas:\n",
      "[ReLU-Tanh] 500 y 300 neuronas - Precisión: 0.9770 - Pérdida: 0.0780 - Tiempo: 30.05s\n",
      "----------------------------------------\n",
      "\n",
      "Comparando para 500 y 500 neuronas:\n",
      "[ReLU-Tanh] 500 y 500 neuronas - Precisión: 0.9778 - Pérdida: 0.0837 - Tiempo: 26.26s\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Modelo con ReLU en la primera capa escondida y tanh en la segunda\n",
    "def ReLU_tanh(hidden_units1, hidden_units2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units1, activation='relu'),\n",
    "        layers.Dense(hidden_units2, activation='tanh'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"[ReLU-Tanh] {hidden_units1} y {hidden_units2} neuronas - Precisión: {val_acc:.4f} - Pérdida: {val_loss:.4f} - Tiempo: {end-start:.2f}s\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# Comparar con diferentes cantidades de neuronas\n",
    "configs = [(200, 100), (300, 200), (500, 300), (500, 500)]\n",
    "for h1, h2 in configs:\n",
    "    print(f\"\\nComparando para {h1} y {h2} neuronas:\")\n",
    "    ReLU_tanh(h1, h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d0447",
   "metadata": {},
   "source": [
    "### 6. Tamaño de Batch Grande\n",
    "Modifique el tamaño de batch a 10,000.\n",
    "- Documente el cambio en el tiempo de entrenamiento\n",
    "- Analice el impacto en la precisión del modelo\n",
    "- Explique teóricamente por qué se observan estos cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c0bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10000 | Neuronas: 200\n",
      "  Precisión de validación final: 0.8937\n",
      "  Pérdida de validación final: 0.3997\n",
      "  Tiempo de entrenamiento: 5.23 segundos\n",
      "----------------------------------------\n",
      "Batch size: 10000 | Neuronas: 500\n",
      "  Precisión de validación final: 0.9119\n",
      "  Pérdida de validación final: 0.3127\n",
      "  Tiempo de entrenamiento: 30.44 segundos\n",
      "----------------------------------------\n",
      "Batch size: 10000 | Neuronas: 1000\n",
      "  Precisión de validación final: 0.9203\n",
      "  Pérdida de validación final: 0.2763\n",
      "  Tiempo de entrenamiento: 11.36 segundos\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9203000068664551, 0.27630770206451416, 11.35644245147705)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entrenar_batch_grande(hidden_units, batch_size=10000):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=batch_size,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Batch size: {batch_size} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y batch grande\n",
    "entrenar_batch_grande(200, batch_size = 10000)\n",
    "# 500 neuronas y batch grande\n",
    "entrenar_batch_grande(500, batch_size = 10000)\n",
    "# 1000 neuronas y batch grande\n",
    "entrenar_batch_grande(1000, batch_size = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154b55f",
   "metadata": {},
   "source": [
    "### 7. Descenso de Gradiente Estocástico (SGD)\n",
    "Ajuste el tamaño de batch a 1 (SGD puro).\n",
    "- Compare el tiempo de ejecución con configuraciones anteriores\n",
    "- Analice la estabilidad y precisión del entrenamiento\n",
    "- Explique si los resultados son coherentes con la teoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84041d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_sgd(hidden_units, batch_size=1):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=batch_size,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Batch size: {batch_size} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(200, batch_size = 1)\n",
    "# 500 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(500, batch_size = 1)\n",
    "# 1000 neuronas y batch=1 (SGD puro)\n",
    "entrenar_sgd(1000, batch_size = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536f6e7",
   "metadata": {},
   "source": [
    "### 8. Tasa de Aprendizaje Baja\n",
    "Modifique la tasa de aprendizaje a 0.0001.\n",
    "- Documente el impacto en la convergencia del modelo\n",
    "- Analice si el modelo alcanza mejor precisión o se queda atrapado en mínimos locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_lr_bajo(hidden_units, lr=0.0001):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Tasa de aprendizaje: {lr} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(200, lr = 0.0001)\n",
    "# 500 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(500, lr = 0.0001)\n",
    "# 1000 neuronas y tasa de aprendizaje baja\n",
    "entrenar_lr_bajo(1000, lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dad08",
   "metadata": {},
   "source": [
    "### 9. Tasa de Aprendizaje Alta \n",
    "Ajuste la tasa de aprendizaje a 0.02.\n",
    "- Documente el impacto en la estabilidad del entrenamiento\n",
    "- Analice si se produce divergencia o mejora en la velocidad de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entrenar_lr_alta(hidden_units, lr=0.02):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    history = model.fit(X_entreno, y_entreno, epochs=5, batch_size=128,\n",
    "                        validation_data=(X_prueba, y_prueba), verbose=0)\n",
    "    end = time.time()\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"Tasa de aprendizaje: {lr} | Neuronas: {hidden_units}\")\n",
    "    print(f\"  Precisión de validación final: {val_acc:.4f}\")\n",
    "    print(f\"  Pérdida de validación final: {val_loss:.4f}\")\n",
    "    print(f\"  Tiempo de entrenamiento: {end - start:.2f} segundos\")\n",
    "    print(\"-\" * 40)\n",
    "    return val_acc, val_loss, end-start\n",
    "\n",
    "# 200 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(200, lr=0.02)\n",
    "# 500 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(500, lr=0.02)\n",
    "# 1000 neuronas y tasa de aprendizaje alta\n",
    "entrenar_lr_alta(1000, lr=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
